import streamlit as st
import os
import sys
import subprocess
import time
import math
import numpy as np
import pandas as pd
import cv2
import torch
import torch.nn as nn
from ultralytics import YOLO
import tempfile
from sklearn.neighbors import NearestNeighbors
import ollama

# ==========================================
# ğŸš¨ ë¼ì´ë¸ŒëŸ¬ë¦¬ ìë™ ì„¤ì¹˜
# ==========================================
try:
    from moviepy.video.io.ImageSequenceClip import ImageSequenceClip
except ImportError:
    try:
        from moviepy.editor import ImageSequenceClip
    except ImportError:
        st.warning("âš ï¸ í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬(moviepy) ì„¤ì¹˜ ì¤‘...")
        subprocess.check_call([sys.executable, "-m", "pip", "install", "moviepy"])
        st.success("âœ… ì„¤ì¹˜ ì™„ë£Œ!")
        time.sleep(1)
        st.rerun()

# ==========================================
# âš™ï¸ ì„¤ì •
# ==========================================
st.set_page_config(page_title="AI 3-Cushion Referee", page_icon="ğŸ±", layout="wide")

MODEL_PATH = "best_model1_lstm.pth"
YOLO_PATH = "best.pt"
TABLE_W, TABLE_H = 1280.0, 720.0
SEQ_LENGTH = 60
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# ğŸ†• [ì„¤ì •] ì‚¬ìš©í•  Ollama ëª¨ë¸ ì´ë¦„
OLLAMA_MODEL = "llama3" 

# ì„±ê³µ ë°ì´í„° DB ë¡œë“œ
try:
    SUCCESS_DB = np.load("success_db.npy", allow_pickle=True)
    DB_START_POS = SUCCESS_DB[:, :2] # ê²€ìƒ‰ìš© ì‹œì‘ì  (ì• 2ê°œ)
    print(f"âœ… ì •ë‹µ DB ë¡œë“œ ì™„ë£Œ: {len(SUCCESS_DB)}ê°œ ë°ì´í„°")
except:
    SUCCESS_DB = None
    print("âš ï¸ ì •ë‹µ DB(success_db.npy)ê°€ ì—†ìŠµë‹ˆë‹¤. RAG ê¸°ëŠ¥ì´ ì œí•œë©ë‹ˆë‹¤.")


# ==========================================
# ğŸ§  ëª¨ë¸ í´ë˜ìŠ¤
# ==========================================
class RichLSTM(nn.Module):
    def __init__(self):
        super().__init__()
        self.lstm = nn.LSTM(6, 64, 2, batch_first=True)
        self.relu = nn.ReLU()
        self.fc = nn.Linear(64, 1)
        self.sigmoid = nn.Sigmoid()
    def forward(self, x):
        out, _ = self.lstm(x)
        out = self.relu(out[:, -1, :])
        return self.sigmoid(self.fc(out))

@st.cache_resource
def load_models():
    model = RichLSTM()
    try:
        state_dict = torch.load(MODEL_PATH, map_location=device)
        model.load_state_dict(state_dict)
        model.to(device)
        model.eval()
    except Exception as e:
        st.error(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None, None
    yolo = YOLO(YOLO_PATH)
    return model, yolo

# ==========================================
# ğŸ†• [í•¨ìˆ˜ ìˆ˜ì •] Ollama ê¸°ë°˜ RAG ì½”ì¹­
# ==========================================
def find_best_ghost_shot(current_start_xy):
    """í˜„ì¬ ê³µ ìœ„ì¹˜ì™€ ê°€ì¥ ê°€ê¹Œìš´ ì„±ê³µ ìƒ· ê¶¤ì  ì°¾ê¸° (Retrieval)"""
    if SUCCESS_DB is None: return None
    
    nbrs = NearestNeighbors(n_neighbors=1, algorithm='ball_tree').fit(DB_START_POS)
    distances, indices = nbrs.kneighbors([current_start_xy])
    
    best_idx = indices[0][0]
    best_shot_data = SUCCESS_DB[best_idx]
    
    trajectory_flat = best_shot_data[2:]
    return trajectory_flat.reshape(-1, 2)

def calculate_angle(trajectory):
    if len(trajectory) < 5: return 0
    dx = trajectory[4][0] - trajectory[0][0]
    dy = trajectory[4][1] - trajectory[0][1]
    deg = math.degrees(math.atan2(dy, dx))
    return abs(deg)

def get_rag_coaching(user_traj_norm, ghost_traj_norm):
    """
    ğŸ†• Ollamaë¥¼ ì‚¬ìš©í•˜ì—¬ ì½”ì¹­ ë©˜íŠ¸ ìƒì„±
    """
    user_angle = calculate_angle(user_traj_norm)
    ghost_angle = calculate_angle(ghost_traj_norm)
    angle_diff = ghost_angle - user_angle
    
    # í”„ë¡¬í”„íŠ¸ (í•œêµ­ì–´ë¡œ ì‘ì„±)
    prompt = f"""
    ë‹¹ì‹ ì€ 3ì¿ ì…˜ ë‹¹êµ¬ ì „ë¬¸ ì½”ì¹˜ì…ë‹ˆë‹¤. ë‹¤ìŒ ë°ì´í„°ë¥¼ ë¶„ì„í•´ ì§§ê³  ëª…í™•í•œ ì¡°ì–¸ì„ í•œ ì¤„ë¡œ í•´ì£¼ì„¸ìš”.
    
    [ìƒí™© ë¶„ì„]
    - ì‚¬ìš©ì ìƒ· ê°ë„: {user_angle:.1f}ë„
    - ì •ë‹µ(AI) ìƒ· ê°ë„: {ghost_angle:.1f}ë„
    - ì°¨ì´: ì‚¬ìš©ìê°€ ì •ë‹µë³´ë‹¤ {abs(angle_diff):.1f}ë„ ë§Œí¼ {"ì‘ê²Œ(ì–‡ê²Œ)" if angle_diff > 0 else "í¬ê²Œ(ë‘ê»ê²Œ)"} ì³¤ìŒ.
    
    [ì§€ì‹œì‚¬í•­]
    - ìœ„ ê°ë„ ì°¨ì´ë¥¼ ì–¸ê¸‰í•˜ë©°, ë‘ê»˜ë‚˜ íšŒì „ì„ ì–´ë–»ê²Œ ì¡°ì ˆí•´ì•¼ ë“ì í•  ìˆ˜ ìˆëŠ”ì§€ ì¡°ì–¸í•˜ì„¸ìš”.
    - ì˜ˆì‹œ: "ë‘ê»˜ê°€ ë„ˆë¬´ ì–‡ì•˜ìŠµë‹ˆë‹¤. ì§„ì…ê°ì„ ì•½ 5ë„ ë” í‚¤ìš°ì„¸ìš”."
    - ë°˜ë§ì´ë‚˜ ì¡´ëŒ“ë§ ìƒê´€ì—†ì´ ì½”ì¹˜ì²˜ëŸ¼ ë§í•˜ì„¸ìš”.
    """
    
    try:
        # ğŸ†• Ollama í˜¸ì¶œ ë¶€ë¶„
        response = ollama.chat(
            model=OLLAMA_MODEL,
            messages=[{'role': 'user', 'content': prompt}]
        )
        return response['message']['content']
    except Exception as e:
        return f"âš ï¸ Ollama í˜¸ì¶œ ì˜¤ë¥˜: {e}. (í„°ë¯¸ë„ì—ì„œ 'ollama serve'ê°€ ì¼œì ¸ ìˆëŠ”ì§€, ëª¨ë¸ëª…ì´ ë§ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”)"

# ==========================================
# ğŸ› ï¸ ë°ì´í„° ì „ì²˜ë¦¬
# ==========================================
def process_data_rich(df):
    white = df[df['cls'] == 1].sort_values('frame')
    yellow = df[df['cls'] == 2].sort_values('frame')
    
    def get_movement_start_frame(ball_df, threshold=2.0):
        if len(ball_df) < 5: return 99999
        ball_df['x_smooth'] = ball_df['x'].rolling(window=3, min_periods=1).mean()
        ball_df['y_smooth'] = ball_df['y'].rolling(window=3, min_periods=1).mean()
        dx = ball_df['x_smooth'].diff().fillna(0)
        dy = ball_df['y_smooth'].diff().fillna(0)
        speed = np.sqrt(dx**2 + dy**2)
        moving_frames = ball_df[speed > threshold]['frame']
        if len(moving_frames) > 0: return moving_frames.min()
        else: return 99999

    w_start = get_movement_start_frame(white)
    y_start = get_movement_start_frame(yellow)
    
    if w_start < y_start: target, ball_color = white, "White"
    elif y_start < w_start: target, ball_color = yellow, "Yellow"
    else: target, ball_color = (white, "White") if len(white) >= len(yellow) else (yellow, "Yellow")

    if len(target) < 10: return None, None, None

    target = target.sort_values('conf', ascending=False).drop_duplicates('frame').sort_values('frame')
    target['x'] = target['x'].rolling(window=5, min_periods=1).mean()
    target['y'] = target['y'].rolling(window=5, min_periods=1).mean()
    
    min_f, max_f = target['frame'].min(), target['frame'].max()
    full_range = range(int(min_f), int(max_f) + 1)
    target = target.set_index('frame').reindex(full_range)
    target[['x', 'y']] = target[['x', 'y']].interpolate(method='linear')
    target = target.dropna().reset_index()

    points_dict = {int(r['frame']): (int(r['x']), int(r['y'])) for _, r in target.iterrows()}

    x_norm = target['x'].values / TABLE_W
    y_norm = target['y'].values / TABLE_H
    
    dx = np.diff(x_norm, prepend=x_norm[0])
    dy = np.diff(y_norm, prepend=y_norm[0])
    speed = np.sqrt(dx**2 + dy**2)
    angle = np.arctan2(dy, dx) / np.pi 

    peak_idx = np.argmax(speed)
    if speed[peak_idx] < 0.005: return None, None, None

    start_idx = max(0, peak_idx - 5)
    end_idx = start_idx + SEQ_LENGTH
    
    features = np.stack([x_norm, y_norm, dx, dy, speed, angle], axis=1)
    features = features[start_idx : end_idx]

    if len(features) < SEQ_LENGTH:
        padding = np.zeros((SEQ_LENGTH - len(features), 6))
        features = np.vstack([features, padding])
        
    mean = features.mean(axis=0)
    std = features.std(axis=0) + 1e-6
    features = (features - mean) / std
    
    return features, points_dict, ball_color

# ==========================================
# ğŸ¬ ë¹„ë””ì˜¤ ì²˜ë¦¬ ë©”ì¸
# ==========================================
def process_video(uploaded_file, ai_referee, yolo_model):
    tfile = tempfile.NamedTemporaryFile(delete=False, suffix='.mp4') 
    tfile.write(uploaded_file.read())
    video_path = tfile.name
    tfile.close()

    cap = cv2.VideoCapture(video_path)
    fps = cap.get(cv2.CAP_PROP_FPS)
    
    frame_list = []
    while True:
        ret, frame = cap.read()
        if not ret: break
        frame_list.append(frame)
    cap.release()
    
    video_data = []
    for i, frame in enumerate(frame_list):
        results = yolo_model.predict(frame, conf=0.05, verbose=False)
        for r in results:
            for box in r.boxes:
                if int(box.cls[0]) in [1, 2]: 
                    x, y, w, h = box.xywh[0].cpu().numpy()
                    video_data.append([i, int(box.cls[0]), x, y, float(box.conf[0])])

    if not video_data: return None, None, None, "ê³µì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.", None

    df = pd.DataFrame(video_data, columns=['frame', 'cls', 'x', 'y', 'conf'])
    input_seq, points_dict, ball_color = process_data_rich(df)
    
    if input_seq is None: return None, None, None, "ìœ íš¨í•œ ê¶¤ì ì´ ì—†ìŠµë‹ˆë‹¤.", None

    input_tensor = torch.FloatTensor(input_seq).unsqueeze(0).to(device)
    with torch.no_grad():
        score = ai_referee(input_tensor).item()
    
    result_text = "SUCCESS" if score >= 0.5 else "FAIL"
    color_res = (0, 255, 0) if score >= 0.5 else (0, 0, 255)
    
    ghost_pts = []
    coaching_msg = None
    
    # ğŸ†• RAG + Ollama ì½”ì¹­
    if result_text == "FAIL" and SUCCESS_DB is not None:
        if len(points_dict) > 0:
            start_frame_idx = min(points_dict.keys())
            start_xy_pixel = points_dict[start_frame_idx]
            start_xy_norm = [start_xy_pixel[0] / TABLE_W, start_xy_pixel[1] / TABLE_H]
            
            ghost_traj_norm = find_best_ghost_shot(start_xy_norm)
            
            if ghost_traj_norm is not None:
                # Ollama ì½”ì¹­ ìƒì„±
                my_traj_norm = input_seq[:, :2]
                coaching_msg = get_rag_coaching(my_traj_norm, ghost_traj_norm)
                
                # ğŸ› ï¸ [í•µì‹¬ ìˆ˜ì •] ì •ë‹µ ê¶¤ì  ë³´ì • (ë‚´ ê³µ ìœ„ì¹˜ë¡œ ì´ë™ + 0 ì œê±°)
                ghost_start_pixel_x = ghost_traj_norm[0][0] * TABLE_W
                ghost_start_pixel_y = ghost_traj_norm[0][1] * TABLE_H
                
                shift_x = start_xy_pixel[0] - ghost_start_pixel_x
                shift_y = start_xy_pixel[1] - ghost_start_pixel_y
                
                for pt in ghost_traj_norm:
                    # íŒ¨ë”©ëœ 0,0 ë°ì´í„°ëŠ” ê±´ë„ˆë›°ê¸° (ì¤‘ìš”!)
                    if pt[0] == 0 and pt[1] == 0: continue
                    
                    # ì›ë˜ ìœ„ì¹˜ë¡œ ë³µì› í›„ ì´ë™ëŸ‰(shift) ë”í•˜ê¸°
                    px = int((pt[0] * TABLE_W) + shift_x)
                    py = int((pt[1] * TABLE_H) + shift_y)
                    
                    # í™”ë©´ ë°–ìœ¼ë¡œ ë„ˆë¬´ ë‚˜ê°€ëŠ” ê°’ ë°©ì§€
                    px = max(0, min(int(TABLE_W), px))
                    py = max(0, min(int(TABLE_H), py))
                    
                    ghost_pts.append((px, py))

    output_frames = []
    draw_pts = []
    
    for i, frame in enumerate(frame_list):
        temp_frame = frame.copy()
        
        if i in points_dict: draw_pts.append(points_dict[i])
        
        cv2.putText(temp_frame, f"Cue Ball: {ball_color}", 
                   (50, 130), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (255, 255, 255), 2)
        
        if len(draw_pts) > 1:
            pts = np.array(draw_pts, np.int32).reshape((-1, 1, 2))
            cv2.polylines(temp_frame, [pts], False, color_res, 3)

        if result_text == "FAIL" and len(ghost_pts) > 1:
            g_pts = np.array(ghost_pts, np.int32).reshape((-1, 1, 2))
            cv2.polylines(temp_frame, [g_pts], False, (0, 255, 127), 2, cv2.LINE_AA)
            end_pt = ghost_pts[-1]
            cv2.putText(temp_frame, "AI Guide", (end_pt[0]-50, end_pt[1]-20), 
                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 127), 2)
            
        frame_rgb = cv2.cvtColor(temp_frame, cv2.COLOR_BGR2RGB)
        output_frames.append(frame_rgb)
    
    timestamp = int(time.time())
    output_filename = f"result_{timestamp}.mp4"
    output_path = os.path.join(os.getcwd(), output_filename)
    
    clip = ImageSequenceClip(output_frames, fps=fps)
    clip.write_videofile(output_path, codec='libx264', audio=False, logger=None, preset='ultrafast')
    
    if os.path.exists(video_path):
        try: os.remove(video_path)
        except: pass
        
    return output_path, result_text, score, None, coaching_msg

# ==========================================
# ğŸ–¥ï¸ UI
# ==========================================
st.title("ğŸ± AI 3-Cushion Referee & Coach")

ai_model, yolo_model = load_models()

uploaded_file = st.file_uploader("ë¶„ì„í•  ì˜ìƒì„ ì—…ë¡œë“œí•˜ì„¸ìš”", type=["mp4", "avi"])

if uploaded_file is not None:
    col1, col2 = st.columns(2)
    with col1:
        st.subheader("ì›ë³¸ ì˜ìƒ")
        st.video(uploaded_file)
        
    with col2:
        st.subheader("íŒë… ê²°ê³¼")
        if st.button("íŒë… ì‹œì‘", type="primary"):
            with st.spinner('AI ì‹¬íŒì´ ì˜ìƒì„ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤...'):
                res_path, res_text, score, err, coach_msg = process_video(uploaded_file, ai_model, yolo_model)
                
                if err:
                    st.error(f"ì˜¤ë¥˜: {err}")
                elif res_path and os.path.exists(res_path):
                    if res_text == "SUCCESS":
                        st.success(f"âœ… íŒë… ê²°ê³¼: ì„±ê³µ ({score*100:.1f}%)")
                    else:
                        st.error(f"âŒ íŒë… ê²°ê³¼: ì‹¤íŒ¨ ({score*100:.1f}%)")
                        if coach_msg:
                            st.info(f"ğŸ¤– **AI ì½”ì¹˜ í”¼ë“œë°± (Ollama):**\n\n{coach_msg}")
                    
                    with open(res_path, 'rb') as f:
                        video_bytes = f.read()
                    
                    if len(video_bytes) > 0:
                        st.video(video_bytes)
                        st.download_button("ê²°ê³¼ ì˜ìƒ ë‹¤ìš´ë¡œë“œ", video_bytes, 
                                         file_name="ai_referee_result.mp4", mime="video/mp4")